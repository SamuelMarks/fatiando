\chapter{Operações com matrizes}
\label{chap:opmat}

A seguir, demonstraremos como fazer algumas operações matemáticas com matrizes e
vetores.
Mas antes, algumas definições.

\section{Definições}

\begin{define}
    Um vetor $\vect{x}$ de $M$ elementos é uma matriz de uma coluna e $M$ linhas

    \begin{equation}
    \vect{x} =
    \begin{bmatrix}
    x_1 \\ x_2 \\ \vdots \\ x_M
    \end{bmatrix}
    \end{equation}
\end{define}

\begin{define}
    Dado um conjunto de $N$ funções $f_1(\vect{x}),\ f_2(\vect{x}),\dotsc,\ f_N(\vect{x})$,
    o vetor de funções $\vect{f}(\vect{x})$ é dado por

    \begin{equation}
    \vect{f}(\vect{x}) =
    \begin{bmatrix}
    f_1(\vect{x}) \\ f_2(\vect{x}) \\ \vdots \\ f_N(\vect{x})
    \end{bmatrix}
    \end{equation}
\end{define}

\begin{define}
    A derivada do vetor $\vect{f}(\vect{x})$ de $N$ elementos em relação ao
    $i$-ésimo elemento de $\vect{x}$, $x_i$, é

    \begin{equation}
    \dfrac{\partial \vect{f}(\vect{x})}{\partial x_i} =
    \begin{bmatrix}
    \dfrac{\partial f_1(\vect{x})}{\partial x_i} \vspace{0.3cm}\\
    \dfrac{\partial f_2(\vect{x})}{\partial x_i} \vspace{0.3cm}\\
    \vdots \vspace{0.3cm}\\
    \dfrac{\partial f_N(\vect{x})}{\partial x_i} \vspace{0.3cm}\\
    \end{bmatrix}
    \end{equation}
\end{define}

\begin{define}
    O vetor $\vect{u}_i^N$ de $N$ elementos possui todos seus elementos
    iguais a zero, exceto o $i$-ésimo elemento que é igual a $1$

    \begin{equation}
    \vect{u}_i^N =
    \begin{bmatrix}
    u_1 \\ \vdots \\ u_{i-1} \\ u_i \\ u_{i+1} \\ \vdots \\ u_N
    \end{bmatrix}=
    \begin{bmatrix}
    0 \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0
    \end{bmatrix}
    \label{eq:ui}
    \end{equation}
\end{define}

\begin{define}
    O operador gradiente $\vect{\nabla}$ em relação ao vetor $\vect{x}$ de $N$
    elementos é igual a

    \begin{equation}
    \vect{\nabla} =
    \begin{bmatrix}
    \dfrac{\partial}{\partial x_1} \vspace{0.3cm}\\
    \dfrac{\partial}{\partial x_2} \vspace{0.3cm}\\ \vdots \vspace{0.3cm}\\
    \dfrac{\partial}{\partial x_N}
    \end{bmatrix}
    \label{eq:opgrad}
    \end{equation}
\end{define}

\begin{define}
    O operador Hessiana $\mat{\nabla}$ em relação ao vetor $\vect{x}$ de $N$
    elementos é igual a

    \begin{equation}
    \mat{\nabla} = \vect{\nabla}\vect{\nabla}^T =
    \begin{bmatrix}
    \dfrac{\partial^2}{\partial x_1^2} &
    \dfrac{\partial^2}{\partial x_2 \partial x_1} &
    \ldots &
    \dfrac{\partial^2}{\partial x_N \partial x_1}
    \vspace{0.3cm}\\
    \dfrac{\partial^2}{\partial x_1 \partial x_2} &
    \dfrac{\partial^2}{\partial x_2^2} &
    \ldots &
    \dfrac{\partial^2}{\partial x_N \partial x_2}
    \vspace{0.3cm}\\
    \vdots & \vdots & \ddots & \vdots
    \vspace{0.3cm}\\
    \dfrac{\partial^2}{\partial x_1 \partial x_N} &
    \dfrac{\partial^2}{\partial x_2 \partial x_N} &
    \ldots &
    \dfrac{\partial^2}{\partial x_N^2}
    \end{bmatrix}
    \label{eq:ophess}
    \end{equation}
\end{define}

\section{Derivadas}

A seguir demonstramos como calcular $\frac{\partial f(\vect{x})}{\partial x_j}$
e $\frac{\partial \vect{f}(\vect{x})}{\partial x_j}$ para diversos casos.
Em todos os casos, $\vect{x}$ é um vetor de $M$ elementos e $\vect{f}(\vect{x})$
é um vetor de $N$ elementos.

\begin{example}
    Para o caso

    \begin{equation}
    \vect{f}(\vect{x}) = \mat{A} \vect{x} ,
    \end{equation}

    \noindent em que $\mat{A}$ é uma matriz de dimensão $N \times M$.
    \\
    \indent
    Seja $\vect{a}_j$ a $j$-ésima coluna de $\mat{A}$, podemos escrever a matriz
    $\mat{A}$ em relação a suas $M$ colunas

    \begin{equation}
    \mat{A} =
        \begin{bmatrix}
        \vect{a}_1 & \ldots & \vect{a}_j & \ldots & \vect{a}_M
        \end{bmatrix} ,
    \end{equation}

    \noindent e então

    \begin{equation}
    \vect{f}(\vect{x}) = x_1\vect{a}_1 + \dotsb + x_j\thinspace\vect{a}_j +
    \dotsb + x_M\vect{a}_M \thinspace .
    \end{equation}

    \indent Neste caso, a derivada de  $\vect{f}(\vect{x})$ em relação
    a $x_j$ é
    
    \begin{equation}
    \begin{split}
    \dfrac{\partial \vect{f}(\vect{x})}{\partial x_j} &=
        \cancelto{0}{\dfrac{\partial x_1\vect{a}_1}{\partial x_j}} + \dotsb +
        \dfrac{\partial x_j\thinspace\vect{a}_j}{\partial x_j} + \dotsb +
        \cancelto{0}{\dfrac{\partial x_M\thinspace\vect{a}_M}{\partial x_j}}
    \\[0.3cm]
    &=
    \vect{a}_j = \mat{A}\thinspace\vect{u}_j^M
    \thinspace .
    \end{split}
    \end{equation}
\end{example}

\begin{example}
    Para o caso

    \begin{equation}
    \vect{f}(\vect{x}) = \vect{x}^T\mat{A}^T  ,
    \end{equation}

    \noindent em que $\mat{A}$ é uma matriz de dimensão $N \times M$.
    \\
    \indent
    Seja $\vect{a}_j$ a $j$-ésima coluna de $\mat{A}$, podemos escrever a matriz
    $\mat{A}^T$ em relação as $M$ colunas de $\mat{A}$

    \begin{equation}
    \mat{A}^T =
        \begin{bmatrix}
        \vect{a}_1^T \\ \vdots \\ \vect{a}_j^T \\ \vdots \\ \vect{a}_M^T
        \end{bmatrix} ,
    \end{equation}

    \noindent e então

    \begin{equation}
    \vect{f}(\vect{x}) = x_1\vect{a}_1^T + \dotsb + x_j\thinspace\vect{a}_j^T +
    \dotsb + x_M\vect{a}_M^T \thinspace .
    \end{equation}

    \indent Neste caso, a derivada de  $\vect{f}(\vect{x})$ em relação
    a $x_j$ é
    
    \begin{equation}
    \begin{split}
    \dfrac{\partial \vect{f}(\vect{x})}{\partial x_j} &=
        \cancelto{0}{\dfrac{\partial x_1\vect{a}_1^T}{\partial x_j}} + \dotsb +
        \dfrac{\partial x_j\thinspace\vect{a}_j^T}{\partial x_j} + \dotsb +
        \cancelto{0}{\dfrac{\partial x_M\thinspace\vect{a}_M^T}{\partial x_j}}
    \\[0.3cm]
    &=
    \vect{a}_j^T
    \thinspace .
    \end{split}
    \end{equation}
\end{example}

\begin{example}
    Para o caso

    \begin{equation}
    \vect{f}(\vect{x}) = \vect{x} ,
    \end{equation}
    
    \noindent a derivada de  $\vect{f}(\vect{x})$ em relação
    a $x_j$ é
    
    \begin{equation}
    \dfrac{\partial \vect{f}(\vect{x})}{\partial x_j} = \vect{u}_j^M
    \thinspace .
    \end{equation}
\end{example}

\begin{example}
    Para o caso\footnote{O transposto de um escalar é igual a ele mesmo.}

    \begin{equation}
    f(\vect{x}) = \vect{a}^T\vect{x} = \vect{x}^T\vect{a},
    \end{equation}
    
    \noindent a derivada de $f(\vect{x})$ em relação
    a $x_j$ é
    
    \begin{equation}
    \dfrac{\partial f(\vect{x})}{\partial x_j} =
        \cancelto{0}{\dfrac{\partial x_1a_1}{\partial x_j}} + \dotsb +
        \dfrac{\partial x_ja_j}{\partial x_j} + \dotsb +
        \cancelto{0}{\dfrac{\partial x_M a_M}{\partial x_j}}
        = a_j = \vect{a}^T \vect{u}_j^M
    \thinspace .
    \label{eq:deriv_aTx}
    \end{equation}
\end{example}

\begin{example}
    Para o caso

    \begin{equation}
    f(\vect{x}) = \vect{x}^T\mat{A}^T\mat{A}\vect{x} =
        (\mat{A}\vect{x})^T(\mat{A}\vect{x}),
    \end{equation}
    
    \noindent a derivada de $f(\vect{x})$ em relação
    a $x_j$ é
    
    \begin{equation}
    \begin{split}
    \dfrac{\partial f(\vect{x})}{\partial x_j} &=
        \left[\dfrac{\partial(\mat{A}\vect{x})}{\partial x_j}\right]^T(\mat{A}\vect{x}) +
        (\mat{A}\vect{x})^T\left[\dfrac{\partial(\mat{A}\vect{x})}{\partial x_j}\right]
    \\[0.3cm]
    &=
    \underbrace{(\mat{A}\vect{u}_j^M)^T(\mat{A}\vect{x})}_{\text{escalar}} + 
    \underbrace{(\mat{A}\vect{x})^T(\mat{A}\vect{u}_j^M)}_{\text{escalar}}
    \thinspace .
    \end{split}
    \end{equation}

    \noindent Como o transposto de um escalar é igual a ele mesmo

    \begin{equation}
    \dfrac{\partial f(\vect{x})}{\partial x_j} =
        2(\mat{A}\vect{u}_j^M)^T(\mat{A}\vect{x}) =
        2(\vect{u}_j^M)^T \mat{A}^T\mat{A}\thinspace\vect{x}
    \thinspace .
    \end{equation}    
\end{example}


\section{Gradientes}

A seguir demonstramos como calcular $\vect{\nabla}f(\vect{x})$
para diversos casos.
Em todos os casos, $\vect{x}$ é um vetor de $M$ elementos.

\begin{example}
    Para o caso\footnote{O transposto de um escalar é igual a ele mesmo.}

    \begin{equation}
    f(\vect{x}) = \vect{a}^T\vect{x} = \vect{x}^T\vect{a},
    \end{equation}
    
    \noindent o gradiente de $f(\vect{x})$ é (ver equação \ref{eq:deriv_aTx})
    
    \begin{equation}
    \vect{\nabla}f(\vect{x}) =
    \begin{bmatrix}
    \dfrac{\partial f(\vect{x})}{\partial x_1} \vspace{0.3cm}\\
    \dfrac{\partial f(\vect{x})}{\partial x_2} \vspace{0.3cm}\\
    \vdots \vspace{0.3cm}\\
    \dfrac{\partial f(\vect{x})}{\partial x_N}
    \end{bmatrix}
    =
    \begin{bmatrix}
    a_1 \\ a_2 \\ \vdots \\ a_N
    \end{bmatrix}
    = \vect{a}
    \thinspace .
    \end{equation}
\end{example}

\begin{example}
    Para o caso

    \begin{equation}
    f(\vect{x}) = \vect{a}^T\vect{x} = \vect{x}^T\vect{a},
    \end{equation}
    
    \noindent o gradiente de $f(\vect{x})$ é (ver equação \ref{eq:deriv_aTx})
    
    \begin{equation}
    \vect{\nabla}f(\vect{x}) =
    \begin{bmatrix}
    \dfrac{\partial f(\vect{x})}{\partial x_1} \vspace{0.3cm}\\
    \dfrac{\partial f(\vect{x})}{\partial x_2} \vspace{0.3cm}\\
    \vdots \vspace{0.3cm}\\
    \dfrac{\partial f(\vect{x})}{\partial x_N}
    \end{bmatrix}
    =
    \begin{bmatrix}
    a_1 \\ a_2 \\ \vdots \\ a_N
    \end{bmatrix}
    = \vect{a}
    \thinspace .
    \end{equation}
\end{example}
